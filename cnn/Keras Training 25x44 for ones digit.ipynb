{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "500000\n",
      "500000\n",
      "x_train shape: (449999, 25, 44, 1)\n",
      "449999 train samples\n",
      "49999 test samples\n",
      "Train on 449999 samples, validate on 49999 samples\n",
      "Epoch 1/8\n",
      "449999/449999 [==============================] - 4016s 9ms/step - loss: 0.3464 - accuracy: 0.8912 - val_loss: 0.0994 - val_accuracy: 0.9701\n",
      "Epoch 2/8\n",
      "449999/449999 [==============================] - 4017s 9ms/step - loss: 0.1179 - accuracy: 0.9641 - val_loss: 0.0653 - val_accuracy: 0.9800\n",
      "Epoch 3/8\n",
      "449999/449999 [==============================] - 4012s 9ms/step - loss: 0.0818 - accuracy: 0.9753 - val_loss: 0.0516 - val_accuracy: 0.9847\n",
      "Epoch 4/8\n",
      "449999/449999 [==============================] - 3927s 9ms/step - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.0407 - val_accuracy: 0.9880\n",
      "Epoch 5/8\n",
      "449999/449999 [==============================] - 3930s 9ms/step - loss: 0.0525 - accuracy: 0.9840 - val_loss: 0.0345 - val_accuracy: 0.9897\n",
      "Epoch 6/8\n",
      "449999/449999 [==============================] - 3913s 9ms/step - loss: 0.0450 - accuracy: 0.9864 - val_loss: 0.0299 - val_accuracy: 0.9909\n",
      "Epoch 7/8\n",
      "449999/449999 [==============================] - 3909s 9ms/step - loss: 0.0398 - accuracy: 0.9879 - val_loss: 0.0283 - val_accuracy: 0.9919\n",
      "Epoch 8/8\n",
      "449999/449999 [==============================] - 3977s 9ms/step - loss: 0.0352 - accuracy: 0.9892 - val_loss: 0.0261 - val_accuracy: 0.9924\n",
      "Test loss: 0.026088435935019024\n",
      "Test accuracy: 0.992439866065979\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 8\n",
    "\n",
    "# input image dimensions\n",
    "#img_rows, img_cols = 28, 28\n",
    "img_rows, img_cols = 25, 44\n",
    "\n",
    "# the data, split between train and test sets\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#if K.image_data_format() == 'channels_first':\n",
    "#    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#    input_shape = (1, img_rows, img_cols)\n",
    "#else:\n",
    "#    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#x_train = x_train.astype('float32')\n",
    "#x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "\n",
    "\n",
    " \n",
    "rpath = \"F:/Google Drive/User_Backup/training\"\n",
    "#wpath = \"F:/Google Drive/User_Backup/training\"\n",
    "h_max, w_max, h_out, w_out = 75, 132, 25, 44\n",
    "image = []\n",
    "answer = []\n",
    "for j in range (0, 100):\n",
    "    print(j)\n",
    "    for k in range (5000):\n",
    "        tpath = rpath + \"/\" + str(j) + \"/\" + str(k) + \".png\"\n",
    "        img = cv2.imread(tpath, 0)\n",
    "        if img.size ==0:\n",
    "            print(tpath)\n",
    "        image.append(img)\n",
    "        answer.append(j%10)\n",
    "        #print(tpath, img.shape)\n",
    "            \n",
    "            \n",
    "print(len(image))\n",
    "print(len(answer))\n",
    "train_image, answer = shuffle(image, answer)\n",
    "i = 0\n",
    "\n",
    "x_train = np.array(train_image[:449999])\n",
    "x_test = np.array(train_image[450000:499999])\n",
    "y_train = np.array(answer[:449999])\n",
    "y_test = np.array(answer[450000:499999])\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#x_train = x_train.reshape(1100, img_rows, img_cols, 1)\n",
    "#x_test = x_test.reshape(1100, img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "from keras.models import load_model\n",
    "model.save(\"F:/Google Drive/User_Backup/model/ones_digit.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 44)\n",
      "449999\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)\n",
    "print(x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-1bfd3aa9b233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"F:/Google Drive/User_Backup/model/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#model.save(\"F:/Google Drive/User_Backup/model/trained.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mjson_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cnn_onese_digit.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0myaml_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_yaml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "model = \"F:/Google Drive/User_Backup/model/\"\n",
    "#model.save(\"F:/Google Drive/User_Backup/model/trained.h5\")\n",
    "json_string = model.to_json()\n",
    "open(os.path.join(model,'cnn_onese_digit.json'), 'w').write(json_string)\n",
    "yaml_string = model.to_yaml()\n",
    "open(os.path.join(model,'cnn_onese_digit.yaml'), 'w').write(yaml_string)\n",
    "print('save weights')\n",
    "model.save_weights(os.path.join(model,'cnn_model_onese_digit.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
